{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition w/ 3D Landmarks\n",
    "\n",
    "Melisa Mete, 150200316\n",
    "\n",
    "Öykü Eren, 150200326\n",
    "\n",
    "Bora Boyacıoğlu, 150200310\n",
    "\n",
    "## Step 1: Data Preparation and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /usr/local/lib/python3.11/site-packages/protobuf-4.25.3-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /usr/local/lib/python3.11/site-packages/protobuf-4.25.3-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 15:34:52.996184: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "from utils.images import Images\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable UserWarning.\n",
    "warnings.filterwarnings(\"ignore\", category = UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading images... 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Load all the images.\n",
    "data_dir = 'data/lfw-deepfunneled/'\n",
    "images = Images()\n",
    "images.read(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying preprocess... 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to all images.\n",
    "images.images = images.apply(preprocess, target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Landmark Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1716208568.733498  586526 gl_context.cc:357] GL version: 2.1 (2.1 INTEL-22.5.10), renderer: Intel(R) Iris(TM) Plus Graphics 645\n",
      "W0000 00:00:1716208568.757660  586526 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1716208568.806710  589085 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1716208568.831137  589086 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# Define the face mesh model.\n",
    "base_options = python.BaseOptions(model_asset_path='face_landmarker_v2_with_blendshapes.task')\n",
    "options = vision.FaceLandmarkerOptions(base_options=base_options,\n",
    "                                       output_face_blendshapes=True,\n",
    "                                       output_facial_transformation_matrixes=True,\n",
    "                                       num_faces=1)\n",
    "detector = vision.FaceLandmarker.create_from_options(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying extract_landmarks... 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Apply the face mesh model to all images.\n",
    "images.landmarks = images.apply(extract_landmarks, detector=detector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Face Realignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To-Do: Implement the face alignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the NumPy arrays.\n",
    "X1 = [np.array(photos) for photos in list(images.images.values())]\n",
    "X2 = [np.array(landmarks) for landmarks in list(images.landmarks.values())]\n",
    "\n",
    "# Create label instances.\n",
    "vocab, y = np.unique(list(images.images.keys()), return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split.\n",
    "splits = train_test_split(X1, X2, y)\n",
    "X1_train, X1_test, X2_train, X2_test, y_train, y_test = splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshapes and NumPy arrays.\n",
    "# Train set.\n",
    "X1_train = np.array([i.reshape(224, 224, 3) for i in X1_train])\n",
    "X2_train = np.array([i.reshape(478, 3) for i in X2_train])\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Test set.\n",
    "X1_test = np.array([i.reshape(224, 224, 3) for i in X1_test])\n",
    "X2_test = np.array([i.reshape(478, 3) for i in X2_test])\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1_train.shape = (9964, 224, 224, 3)\n",
      "X2_train.shape = (9964, 478, 3)\n",
      "y_train.shape = (9964,)\n",
      "X1_test.shape = (3269, 224, 224, 3)\n",
      "X2_test.shape = (3269, 478, 3)\n",
      "y_test.shape = (3269,)\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes.\n",
    "print(f\"{X1_train.shape = }\",\n",
    "      f\"{X2_train.shape = }\",\n",
    "      f\"{y_train.shape = }\",\n",
    "      f\"{X1_test.shape = }\",\n",
    "      f\"{X2_test.shape = }\",\n",
    "      f\"{y_test.shape = }\",\n",
    "      sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the NumPy arrays.\n",
    "np.save('data/arr/X1_train.npy', X1_train)\n",
    "np.save('data/arr/X2_train.npy', X2_train)\n",
    "np.save('data/arr/y_train.npy', y_train)\n",
    "np.save('data/arr/X1_test.npy', X1_test)\n",
    "np.save('data/arr/X2_test.npy', X2_test)\n",
    "np.save('data/arr/y_test.npy', y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
