{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 13:00:18.869172: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the augmented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = pd.read_json('new_data/lm_300wLP_anno_tr.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the X array.\n",
    "X = []\n",
    "for row in lm['landmarks_2d']:\n",
    "    \n",
    "    # Skip the row if it has less than 12 elements.\n",
    "    if len(row) < 12:\n",
    "        continue\n",
    "    \n",
    "    # Get the first 12 elements of the array.\n",
    "    row = row[:12]\n",
    "    \n",
    "    # Flatten the array.\n",
    "    row_ = np.array(row).reshape(12, 68 * 2)\n",
    "    X.append(row_)\n",
    "\n",
    "# Create the y array.\n",
    "y = []\n",
    "for row in lm['landmarks']:\n",
    "    \n",
    "    # Skip the row if it has less than 12 elements.\n",
    "    if len(row) < 12:\n",
    "        continue\n",
    "    \n",
    "    # Get only the first element.\n",
    "    row = row[0]\n",
    "    \n",
    "    # Flatten the array.\n",
    "    row_ = np.array(row).reshape(68 * 3)\n",
    "    y.append(row_)\n",
    "\n",
    "# Convert the lists to numpy arrays.\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "y_train, y_test = train_test_split(y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarkModel(models.Model):\n",
    "    def __init__(self, in_, out_):\n",
    "        super(LandmarkModel, self).__init__()\n",
    "        self.lstm1 = layers.LSTM(64, return_sequences=True, input_shape=in_)\n",
    "        self.bn1 = layers.BatchNormalization()  # Batch normalization after first LSTM\n",
    "        self.dropout1 = layers.Dropout(0.3)  # Dropout after batch normalization\n",
    "\n",
    "        self.lstm2 = layers.LSTM(128, return_sequences=False)\n",
    "        self.bn2 = layers.BatchNormalization()  # Batch normalization after second LSTM\n",
    "        self.dropout2 = layers.Dropout(0.3)  # Dropout after batch normalization\n",
    "\n",
    "        self.dense = layers.Dense(out_, activation='linear')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.lstm1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.lstm2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        return self.dense(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X[0].shape=(12, 136), y[0].shape=(204,)\n"
     ]
    }
   ],
   "source": [
    "# Get the input and output shapes.\n",
    "in_ = X[0].shape\n",
    "out_ = y[0].shape[0]\n",
    "\n",
    "# Print the shapes.\n",
    "print(f'{X[0].shape=}, {y[0].shape=}')\n",
    "\n",
    "# Create the model.\n",
    "model = LandmarkModel(in_, out_)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae', 'mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 11s 42ms/step - loss: 47651.0625 - mae: 184.4454 - mse: 47651.0625 - val_loss: 47742.2500 - val_mae: 184.7350 - val_mse: 47742.2500\n",
      "Epoch 2/120\n",
      "75/75 [==============================] - 1s 19ms/step - loss: 47206.5078 - mae: 183.3980 - mse: 47206.5078 - val_loss: 46675.9883 - val_mae: 182.2025 - val_mse: 46675.9883\n",
      "Epoch 3/120\n",
      "75/75 [==============================] - 1s 19ms/step - loss: 45666.2188 - mae: 179.6288 - mse: 45666.2188 - val_loss: 44153.3164 - val_mae: 175.9357 - val_mse: 44153.3164\n",
      "Epoch 4/120\n",
      "75/75 [==============================] - 1s 18ms/step - loss: 42612.3281 - mae: 171.9545 - mse: 42612.3281 - val_loss: 40755.3359 - val_mae: 167.2959 - val_mse: 40755.3359\n",
      "Epoch 5/120\n",
      "75/75 [==============================] - 1s 19ms/step - loss: 38458.5781 - mae: 161.4511 - mse: 38458.5781 - val_loss: 36409.6289 - val_mae: 156.2797 - val_mse: 36409.6289\n",
      "Epoch 6/120\n",
      "75/75 [==============================] - 1s 18ms/step - loss: 33549.8203 - mae: 149.1699 - mse: 33549.8203 - val_loss: 32570.6348 - val_mae: 146.8138 - val_mse: 32570.6348\n",
      "Epoch 7/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 28367.8574 - mae: 136.0470 - mse: 28367.8574 - val_loss: 31506.3613 - val_mae: 144.7789 - val_mse: 31506.3613\n",
      "Epoch 8/120\n",
      "75/75 [==============================] - 2s 20ms/step - loss: 23154.4824 - mae: 121.9667 - mse: 23154.4824 - val_loss: 24081.6113 - val_mae: 124.9253 - val_mse: 24081.6113\n",
      "Epoch 9/120\n",
      "75/75 [==============================] - 1s 19ms/step - loss: 18306.9902 - mae: 107.5108 - mse: 18306.9902 - val_loss: 20362.9883 - val_mae: 114.3199 - val_mse: 20362.9883\n",
      "Epoch 10/120\n",
      "75/75 [==============================] - 2s 21ms/step - loss: 13981.7207 - mae: 92.8395 - mse: 13981.7207 - val_loss: 14391.9258 - val_mae: 94.7128 - val_mse: 14391.9258\n",
      "Epoch 11/120\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 10337.5967 - mae: 78.4695 - mse: 10337.5967 - val_loss: 6444.1606 - val_mae: 60.3379 - val_mse: 6444.1606\n",
      "Epoch 12/120\n",
      "75/75 [==============================] - 2s 20ms/step - loss: 7401.0352 - mae: 64.9872 - mse: 7401.0352 - val_loss: 4182.5137 - val_mae: 47.2734 - val_mse: 4182.5137\n",
      "Epoch 13/120\n",
      "75/75 [==============================] - 1s 19ms/step - loss: 5235.5791 - mae: 53.4925 - mse: 5235.5791 - val_loss: 6667.9248 - val_mae: 62.4344 - val_mse: 6667.9248\n",
      "Epoch 14/120\n",
      "75/75 [==============================] - 2s 20ms/step - loss: 3610.4534 - mae: 43.5366 - mse: 3610.4534 - val_loss: 3491.3411 - val_mae: 43.2408 - val_mse: 3491.3411\n",
      "Epoch 15/120\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 2482.0164 - mae: 35.6241 - mse: 2482.0164 - val_loss: 1721.4463 - val_mae: 29.3872 - val_mse: 1721.4463\n",
      "Epoch 16/120\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 1750.2125 - mae: 29.8220 - mse: 1750.2125 - val_loss: 1507.3633 - val_mae: 27.7540 - val_mse: 1507.3633\n",
      "Epoch 17/120\n",
      "75/75 [==============================] - 2s 20ms/step - loss: 1268.8795 - mae: 25.5943 - mse: 1268.8795 - val_loss: 1357.5017 - val_mae: 26.6647 - val_mse: 1357.5017\n",
      "Epoch 18/120\n",
      "75/75 [==============================] - 1s 18ms/step - loss: 943.2994 - mae: 22.4306 - mse: 943.2994 - val_loss: 2667.5181 - val_mae: 40.3115 - val_mse: 2667.5181\n",
      "Epoch 19/120\n",
      "75/75 [==============================] - 2s 20ms/step - loss: 764.7615 - mae: 20.4951 - mse: 764.7615 - val_loss: 1570.4510 - val_mae: 30.5509 - val_mse: 1570.4510\n",
      "Epoch 20/120\n",
      "75/75 [==============================] - 2s 20ms/step - loss: 617.2361 - mae: 18.6487 - mse: 617.2361 - val_loss: 777.1182 - val_mae: 21.1579 - val_mse: 777.1182\n",
      "Epoch 21/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 538.0397 - mae: 17.5738 - mse: 538.0397 - val_loss: 496.0174 - val_mae: 16.9790 - val_mse: 496.0174\n",
      "Epoch 22/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 488.1886 - mae: 16.7981 - mse: 488.1886 - val_loss: 285.2872 - val_mae: 13.0092 - val_mse: 285.2872\n",
      "Epoch 23/120\n",
      "75/75 [==============================] - 2s 21ms/step - loss: 446.5168 - mae: 16.1405 - mse: 446.5168 - val_loss: 281.8376 - val_mae: 12.8751 - val_mse: 281.8376\n",
      "Epoch 24/120\n",
      "75/75 [==============================] - 2s 20ms/step - loss: 437.2766 - mae: 15.9906 - mse: 437.2766 - val_loss: 452.8601 - val_mae: 16.8895 - val_mse: 452.8601\n",
      "Epoch 25/120\n",
      "75/75 [==============================] - 2s 21ms/step - loss: 412.5464 - mae: 15.5813 - mse: 412.5464 - val_loss: 793.4435 - val_mae: 23.0878 - val_mse: 793.4435\n",
      "Epoch 26/120\n",
      "75/75 [==============================] - 1s 19ms/step - loss: 407.1652 - mae: 15.4354 - mse: 407.1652 - val_loss: 486.5756 - val_mae: 17.8635 - val_mse: 486.5756\n",
      "Epoch 27/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 408.1582 - mae: 15.4802 - mse: 408.1582 - val_loss: 242.1947 - val_mae: 11.7829 - val_mse: 242.1947\n",
      "Epoch 28/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 401.0857 - mae: 15.3697 - mse: 401.0857 - val_loss: 428.3962 - val_mae: 16.7507 - val_mse: 428.3962\n",
      "Epoch 29/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 395.7103 - mae: 15.2755 - mse: 395.7103 - val_loss: 322.6424 - val_mae: 14.1821 - val_mse: 322.6424\n",
      "Epoch 30/120\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 396.4744 - mae: 15.2542 - mse: 396.4744 - val_loss: 685.1716 - val_mae: 21.7265 - val_mse: 685.1716\n",
      "Epoch 31/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 383.5670 - mae: 15.0724 - mse: 383.5670 - val_loss: 265.9580 - val_mae: 12.5661 - val_mse: 265.9580\n",
      "Epoch 32/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 391.4547 - mae: 15.2060 - mse: 391.4547 - val_loss: 237.9557 - val_mae: 11.6045 - val_mse: 237.9557\n",
      "Epoch 33/120\n",
      "75/75 [==============================] - 2s 21ms/step - loss: 389.7209 - mae: 15.1927 - mse: 389.7209 - val_loss: 234.7379 - val_mae: 11.4989 - val_mse: 234.7379\n",
      "Epoch 34/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 382.6999 - mae: 15.0220 - mse: 382.6999 - val_loss: 242.8653 - val_mae: 11.7795 - val_mse: 242.8653\n",
      "Epoch 35/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 389.0881 - mae: 15.2164 - mse: 389.0881 - val_loss: 242.6909 - val_mae: 11.7757 - val_mse: 242.6909\n",
      "Epoch 36/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 393.6402 - mae: 15.2384 - mse: 393.6402 - val_loss: 219.5374 - val_mae: 10.9075 - val_mse: 219.5374\n",
      "Epoch 37/120\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 392.6216 - mae: 15.1849 - mse: 392.6216 - val_loss: 292.2540 - val_mae: 13.3737 - val_mse: 292.2540\n",
      "Epoch 38/120\n",
      "75/75 [==============================] - 1s 19ms/step - loss: 383.5353 - mae: 15.0269 - mse: 383.5353 - val_loss: 360.4953 - val_mae: 15.2855 - val_mse: 360.4953\n",
      "Epoch 39/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 392.9783 - mae: 15.2333 - mse: 392.9783 - val_loss: 230.7829 - val_mae: 11.3299 - val_mse: 230.7829\n",
      "Epoch 40/120\n",
      "75/75 [==============================] - 2s 20ms/step - loss: 378.8410 - mae: 14.9450 - mse: 378.8410 - val_loss: 239.5993 - val_mae: 11.6725 - val_mse: 239.5993\n",
      "Epoch 41/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 383.2615 - mae: 15.0259 - mse: 383.2615 - val_loss: 223.2449 - val_mae: 11.0494 - val_mse: 223.2449\n",
      "Epoch 42/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 390.8937 - mae: 15.2209 - mse: 390.8937 - val_loss: 244.7686 - val_mae: 11.8405 - val_mse: 244.7686\n",
      "Epoch 43/120\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 400.0511 - mae: 15.3541 - mse: 400.0511 - val_loss: 224.7053 - val_mae: 11.1074 - val_mse: 224.7053\n",
      "Epoch 44/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 381.4353 - mae: 15.0199 - mse: 381.4353 - val_loss: 219.9059 - val_mae: 10.9071 - val_mse: 219.9059\n",
      "Epoch 45/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 385.4986 - mae: 15.0908 - mse: 385.4986 - val_loss: 231.9932 - val_mae: 11.3710 - val_mse: 231.9932\n",
      "Epoch 46/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 398.2739 - mae: 15.2873 - mse: 398.2739 - val_loss: 331.6091 - val_mae: 14.4896 - val_mse: 331.6091\n",
      "Epoch 47/120\n",
      "75/75 [==============================] - 2s 21ms/step - loss: 389.5142 - mae: 15.2048 - mse: 389.5142 - val_loss: 266.1552 - val_mae: 12.5592 - val_mse: 266.1552\n",
      "Epoch 48/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 396.3295 - mae: 15.2643 - mse: 396.3295 - val_loss: 249.7877 - val_mae: 12.0185 - val_mse: 249.7877\n",
      "Epoch 49/120\n",
      "75/75 [==============================] - 2s 21ms/step - loss: 386.7896 - mae: 15.1086 - mse: 386.7896 - val_loss: 240.4624 - val_mae: 11.6845 - val_mse: 240.4624\n",
      "Epoch 50/120\n",
      "75/75 [==============================] - 2s 21ms/step - loss: 379.8483 - mae: 14.9141 - mse: 379.8483 - val_loss: 233.8097 - val_mae: 11.4362 - val_mse: 233.8097\n",
      "Epoch 51/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 386.1749 - mae: 15.0506 - mse: 386.1749 - val_loss: 262.6302 - val_mae: 12.4373 - val_mse: 262.6302\n",
      "Epoch 52/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 386.8251 - mae: 15.0932 - mse: 386.8251 - val_loss: 238.8769 - val_mae: 11.6492 - val_mse: 238.8769\n",
      "Epoch 53/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 394.8604 - mae: 15.2791 - mse: 394.8604 - val_loss: 222.0983 - val_mae: 10.9971 - val_mse: 222.0983\n",
      "Epoch 54/120\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 389.5689 - mae: 15.1047 - mse: 389.5689 - val_loss: 238.4505 - val_mae: 11.6375 - val_mse: 238.4505\n",
      "Epoch 55/120\n",
      "75/75 [==============================] - 2s 21ms/step - loss: 387.5190 - mae: 15.1283 - mse: 387.5190 - val_loss: 219.8778 - val_mae: 10.9035 - val_mse: 219.8778\n",
      "Epoch 56/120\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 394.2199 - mae: 15.2258 - mse: 394.2199 - val_loss: 241.1088 - val_mae: 11.7333 - val_mse: 241.1088\n",
      "Epoch 57/120\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 389.4717 - mae: 15.1794 - mse: 389.4717 - val_loss: 255.7381 - val_mae: 12.2566 - val_mse: 255.7381\n",
      "Epoch 58/120\n",
      "75/75 [==============================] - 2s 21ms/step - loss: 388.5143 - mae: 15.1652 - mse: 388.5143 - val_loss: 316.6468 - val_mae: 14.1449 - val_mse: 316.6468\n",
      "Epoch 59/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 391.9763 - mae: 15.1880 - mse: 391.9763 - val_loss: 369.2451 - val_mae: 15.5085 - val_mse: 369.2451\n",
      "Epoch 60/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 405.7552 - mae: 15.4122 - mse: 405.7552 - val_loss: 220.6112 - val_mae: 10.9413 - val_mse: 220.6112\n",
      "Epoch 61/120\n",
      "75/75 [==============================] - 2s 21ms/step - loss: 390.2158 - mae: 15.2089 - mse: 390.2158 - val_loss: 247.3099 - val_mae: 11.9677 - val_mse: 247.3099\n",
      "Epoch 62/120\n",
      "75/75 [==============================] - 2s 20ms/step - loss: 394.7974 - mae: 15.2450 - mse: 394.7974 - val_loss: 289.1052 - val_mae: 13.3470 - val_mse: 289.1052\n",
      "Epoch 63/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 381.8662 - mae: 15.0376 - mse: 381.8662 - val_loss: 265.7823 - val_mae: 12.6133 - val_mse: 265.7823\n",
      "Epoch 64/120\n",
      "75/75 [==============================] - 2s 20ms/step - loss: 397.0413 - mae: 15.3165 - mse: 397.0413 - val_loss: 221.4742 - val_mae: 10.9841 - val_mse: 221.4742\n",
      "Epoch 65/120\n",
      "75/75 [==============================] - 1s 19ms/step - loss: 384.4346 - mae: 15.0407 - mse: 384.4346 - val_loss: 219.6383 - val_mae: 10.9123 - val_mse: 219.6383\n",
      "Epoch 66/120\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 390.0369 - mae: 15.1780 - mse: 390.0369 - val_loss: 243.7513 - val_mae: 11.7916 - val_mse: 243.7513\n",
      "Epoch 67/120\n",
      "75/75 [==============================] - 2s 21ms/step - loss: 384.0476 - mae: 15.0289 - mse: 384.0476 - val_loss: 226.2826 - val_mae: 11.1584 - val_mse: 226.2826\n",
      "Epoch 68/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 392.3576 - mae: 15.2509 - mse: 392.3576 - val_loss: 221.1450 - val_mae: 10.9597 - val_mse: 221.1450\n",
      "Epoch 69/120\n",
      "75/75 [==============================] - 2s 20ms/step - loss: 397.0675 - mae: 15.3267 - mse: 397.0675 - val_loss: 225.2285 - val_mae: 11.1193 - val_mse: 225.2285\n",
      "Epoch 70/120\n",
      "75/75 [==============================] - 2s 20ms/step - loss: 384.1189 - mae: 15.0466 - mse: 384.1189 - val_loss: 230.2070 - val_mae: 11.3287 - val_mse: 230.2070\n",
      "Epoch 71/120\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 389.8853 - mae: 15.1382 - mse: 389.8853 - val_loss: 219.3345 - val_mae: 10.8851 - val_mse: 219.3345\n",
      "Epoch 72/120\n",
      "75/75 [==============================] - 1s 19ms/step - loss: 389.4669 - mae: 15.2064 - mse: 389.4669 - val_loss: 221.5898 - val_mae: 10.9811 - val_mse: 221.5898\n",
      "Epoch 73/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 384.7433 - mae: 15.0644 - mse: 384.7433 - val_loss: 219.0541 - val_mae: 10.8791 - val_mse: 219.0541\n",
      "Epoch 74/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 380.7560 - mae: 15.0077 - mse: 380.7560 - val_loss: 219.3972 - val_mae: 10.8871 - val_mse: 219.3972\n",
      "Epoch 75/120\n",
      "75/75 [==============================] - 2s 21ms/step - loss: 387.0905 - mae: 15.1024 - mse: 387.0905 - val_loss: 235.5133 - val_mae: 11.5208 - val_mse: 235.5133\n",
      "Epoch 76/120\n",
      "75/75 [==============================] - 2s 20ms/step - loss: 393.7622 - mae: 15.2109 - mse: 393.7622 - val_loss: 224.5096 - val_mae: 11.1119 - val_mse: 224.5096\n",
      "Epoch 77/120\n",
      "75/75 [==============================] - 2s 21ms/step - loss: 390.7761 - mae: 15.1736 - mse: 390.7761 - val_loss: 219.5389 - val_mae: 10.8880 - val_mse: 219.5389\n",
      "Epoch 78/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 386.5424 - mae: 15.1748 - mse: 386.5424 - val_loss: 237.1059 - val_mae: 11.5665 - val_mse: 237.1059\n",
      "Epoch 79/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 393.5628 - mae: 15.1877 - mse: 393.5628 - val_loss: 233.4354 - val_mae: 11.4281 - val_mse: 233.4354\n",
      "Epoch 80/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 384.0548 - mae: 15.0948 - mse: 384.0548 - val_loss: 261.4033 - val_mae: 12.4588 - val_mse: 261.4033\n",
      "Epoch 81/120\n",
      "75/75 [==============================] - 2s 21ms/step - loss: 391.1918 - mae: 15.1624 - mse: 391.1918 - val_loss: 236.3800 - val_mae: 11.5877 - val_mse: 236.3800\n",
      "Epoch 82/120\n",
      "75/75 [==============================] - 2s 21ms/step - loss: 386.6792 - mae: 15.0631 - mse: 386.6792 - val_loss: 221.1329 - val_mae: 10.9587 - val_mse: 221.1329\n",
      "Epoch 83/120\n",
      "75/75 [==============================] - 2s 20ms/step - loss: 394.5357 - mae: 15.2351 - mse: 394.5357 - val_loss: 223.1838 - val_mae: 11.0460 - val_mse: 223.1838\n",
      "Epoch 84/120\n",
      "75/75 [==============================] - 2s 20ms/step - loss: 379.7673 - mae: 14.9435 - mse: 379.7673 - val_loss: 231.6188 - val_mae: 11.3549 - val_mse: 231.6188\n",
      "Epoch 85/120\n",
      "75/75 [==============================] - 2s 20ms/step - loss: 393.2495 - mae: 15.2490 - mse: 393.2495 - val_loss: 245.2290 - val_mae: 11.8470 - val_mse: 245.2290\n",
      "Epoch 86/120\n",
      "75/75 [==============================] - 2s 21ms/step - loss: 392.2102 - mae: 15.2087 - mse: 392.2102 - val_loss: 219.7975 - val_mae: 10.9057 - val_mse: 219.7975\n",
      "Epoch 87/120\n",
      "75/75 [==============================] - 2s 20ms/step - loss: 386.0563 - mae: 15.0555 - mse: 386.0563 - val_loss: 220.1157 - val_mae: 10.9267 - val_mse: 220.1157\n",
      "Epoch 88/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 382.5885 - mae: 15.0158 - mse: 382.5885 - val_loss: 232.5295 - val_mae: 11.3956 - val_mse: 232.5295\n",
      "Epoch 89/120\n",
      "75/75 [==============================] - 2s 20ms/step - loss: 392.1590 - mae: 15.1854 - mse: 392.1590 - val_loss: 224.1851 - val_mae: 11.0489 - val_mse: 224.1851\n",
      "Epoch 90/120\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 397.3115 - mae: 15.3119 - mse: 397.3115 - val_loss: 713.3060 - val_mae: 22.2572 - val_mse: 713.3060\n",
      "Epoch 91/120\n",
      "75/75 [==============================] - 2s 21ms/step - loss: 390.2024 - mae: 15.1811 - mse: 390.2024 - val_loss: 347.6954 - val_mae: 14.9345 - val_mse: 347.6954\n",
      "Epoch 92/120\n",
      "75/75 [==============================] - 1s 18ms/step - loss: 387.3116 - mae: 15.0655 - mse: 387.3116 - val_loss: 220.7223 - val_mae: 10.9446 - val_mse: 220.7223\n",
      "Epoch 93/120\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 392.3994 - mae: 15.2384 - mse: 392.3994 - val_loss: 226.1916 - val_mae: 11.1590 - val_mse: 226.1916\n",
      "Epoch 94/120\n",
      "75/75 [==============================] - 2s 26ms/step - loss: 386.0531 - mae: 15.0272 - mse: 386.0531 - val_loss: 224.0336 - val_mae: 11.0836 - val_mse: 224.0336\n",
      "Epoch 95/120\n",
      "75/75 [==============================] - 2s 21ms/step - loss: 386.1108 - mae: 15.0804 - mse: 386.1108 - val_loss: 219.2985 - val_mae: 10.8805 - val_mse: 219.2985\n",
      "Epoch 96/120\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 396.7719 - mae: 15.2944 - mse: 396.7719 - val_loss: 219.9606 - val_mae: 10.9231 - val_mse: 219.9606\n",
      "Epoch 97/120\n",
      "75/75 [==============================] - 2s 21ms/step - loss: 386.0899 - mae: 15.0879 - mse: 386.0899 - val_loss: 223.1346 - val_mae: 11.0394 - val_mse: 223.1346\n",
      "Epoch 98/120\n",
      "75/75 [==============================] - 2s 20ms/step - loss: 383.8718 - mae: 15.0000 - mse: 383.8718 - val_loss: 219.2864 - val_mae: 10.8732 - val_mse: 219.2864\n",
      "Epoch 99/120\n",
      "75/75 [==============================] - 2s 20ms/step - loss: 386.4739 - mae: 15.1503 - mse: 386.4739 - val_loss: 220.6362 - val_mae: 10.9465 - val_mse: 220.6362\n",
      "Epoch 100/120\n",
      "75/75 [==============================] - 2s 20ms/step - loss: 389.1869 - mae: 15.1692 - mse: 389.1869 - val_loss: 221.2227 - val_mae: 10.9706 - val_mse: 221.2227\n",
      "Epoch 101/120\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 380.4731 - mae: 15.0389 - mse: 380.4731 - val_loss: 219.2104 - val_mae: 10.8901 - val_mse: 219.2104\n",
      "Epoch 102/120\n",
      "75/75 [==============================] - 2s 20ms/step - loss: 392.5273 - mae: 15.1894 - mse: 392.5273 - val_loss: 223.2178 - val_mae: 11.0463 - val_mse: 223.2178\n",
      "Epoch 103/120\n",
      "75/75 [==============================] - 2s 20ms/step - loss: 384.6516 - mae: 15.0559 - mse: 384.6516 - val_loss: 219.0836 - val_mae: 10.8879 - val_mse: 219.0836\n",
      "Epoch 104/120\n",
      "75/75 [==============================] - 2s 21ms/step - loss: 384.8984 - mae: 15.0694 - mse: 384.8984 - val_loss: 219.3909 - val_mae: 10.8948 - val_mse: 219.3909\n",
      "Epoch 105/120\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 392.9001 - mae: 15.2792 - mse: 392.9001 - val_loss: 219.8523 - val_mae: 10.9153 - val_mse: 219.8523\n",
      "Epoch 106/120\n",
      "75/75 [==============================] - 1s 17ms/step - loss: 380.7539 - mae: 14.9972 - mse: 380.7539 - val_loss: 220.5531 - val_mae: 10.9361 - val_mse: 220.5531\n",
      "Epoch 107/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 390.6527 - mae: 15.1782 - mse: 390.6527 - val_loss: 221.9380 - val_mae: 10.9983 - val_mse: 221.9380\n",
      "Epoch 108/120\n",
      "75/75 [==============================] - 2s 21ms/step - loss: 381.3333 - mae: 15.0269 - mse: 381.3333 - val_loss: 219.2228 - val_mae: 10.8669 - val_mse: 219.2228\n",
      "Epoch 109/120\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 398.2578 - mae: 15.2698 - mse: 398.2578 - val_loss: 219.0757 - val_mae: 10.8798 - val_mse: 219.0757\n",
      "Epoch 110/120\n",
      "75/75 [==============================] - 2s 21ms/step - loss: 389.4405 - mae: 15.2220 - mse: 389.4405 - val_loss: 222.2361 - val_mae: 11.0272 - val_mse: 222.2361\n",
      "Epoch 111/120\n",
      "75/75 [==============================] - 1s 18ms/step - loss: 385.5259 - mae: 15.0510 - mse: 385.5259 - val_loss: 221.7358 - val_mae: 10.9695 - val_mse: 221.7358\n",
      "Epoch 112/120\n",
      "75/75 [==============================] - 1s 18ms/step - loss: 387.9616 - mae: 15.1637 - mse: 387.9616 - val_loss: 224.3014 - val_mae: 11.0972 - val_mse: 224.3014\n",
      "Epoch 113/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 388.4933 - mae: 15.1559 - mse: 388.4933 - val_loss: 221.0331 - val_mae: 10.9473 - val_mse: 221.0331\n",
      "Epoch 114/120\n",
      "75/75 [==============================] - 2s 20ms/step - loss: 384.0996 - mae: 15.0641 - mse: 384.0996 - val_loss: 220.0593 - val_mae: 10.9222 - val_mse: 220.0593\n",
      "Epoch 115/120\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 395.8632 - mae: 15.2896 - mse: 395.8632 - val_loss: 219.2426 - val_mae: 10.8998 - val_mse: 219.2426\n",
      "Epoch 116/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 393.6459 - mae: 15.2364 - mse: 393.6459 - val_loss: 219.4397 - val_mae: 10.8941 - val_mse: 219.4397\n",
      "Epoch 117/120\n",
      "75/75 [==============================] - 2s 20ms/step - loss: 397.2596 - mae: 15.2824 - mse: 397.2596 - val_loss: 219.9242 - val_mae: 10.9062 - val_mse: 219.9242\n",
      "Epoch 118/120\n",
      "75/75 [==============================] - 2s 21ms/step - loss: 382.1711 - mae: 15.0183 - mse: 382.1711 - val_loss: 219.1331 - val_mae: 10.8791 - val_mse: 219.1331\n",
      "Epoch 119/120\n",
      "75/75 [==============================] - 2s 21ms/step - loss: 385.0137 - mae: 15.0557 - mse: 385.0137 - val_loss: 219.4566 - val_mae: 10.8900 - val_mse: 219.4566\n",
      "Epoch 120/120\n",
      "75/75 [==============================] - 1s 20ms/step - loss: 391.0486 - mae: 15.1159 - mse: 391.0486 - val_loss: 219.8502 - val_mae: 10.9041 - val_mse: 219.8502\n"
     ]
    }
   ],
   "source": [
    "# Train the model.\n",
    "history = model.fit(X_train, y_train, epochs=120, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 11ms/step - loss: 235.6573 - mae: 11.3266 - mse: 235.6573\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model.\n",
    "loss = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/2024-04-24_13-09-38/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/2024-04-24_13-09-38/assets\n"
     ]
    }
   ],
   "source": [
    "# Get the current timestamp.\n",
    "now = dt.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "# Save the model.\n",
    "model.save(f'models/{now}', save_format='tf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
